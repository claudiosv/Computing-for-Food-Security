{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1901cea0-9243-44ee-98aa-e14e36d9c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "import subprocess\n",
    "import urllib\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import requests\n",
    "from c_usda_quick_stats import c_usda_quick_stats\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c1ec8d-ff9f-465c-98fc-295ee8eb0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_NASS_API_key = \"A269B59D-8921-3BAB-B00A-26507C5E9D29\"\n",
    "\n",
    "def curr_timestamp():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    return formatted_datetime\n",
    "\n",
    "\n",
    "def get_coordinate_pixel(tiff_file, lon, lat):\n",
    "    dataset = rasterio.open(tiff_file)\n",
    "    py, px = dataset.index(lon, lat)\n",
    "\n",
    "    window = rasterio.windows.Window(px, py, 1, 1)\n",
    "\n",
    "    clip = dataset.read(window=window)\n",
    "\n",
    "    return clip[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287678f2-7487-4fbf-9881-f48ed9d373b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"USDA-NASS--v01/OUTPUTS/\")\n",
    "\n",
    "archive_dir = Path(\"ML-ARCHIVES--v01/\")\n",
    "\n",
    "tif_dir = Path(\"GAEZ-SOIL-for-ML/OUTPUTS/\")\n",
    "\n",
    "weather_dir = archive_dir / \"WEATHER-DATA--v01/\"\n",
    "\n",
    "ml_tables_dir = archive_dir / \"ML-TABLES--v01/\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "tif_dir.mkdir(parents=True, exist_ok=True)\n",
    "weather_dir.mkdir(parents=True, exist_ok=True)\n",
    "ml_tables_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50c1b86-9209-4368-bd91-8d53d6fd99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_survey_1997_file = output_dir / \"national_farm_survey_acres_ge_1997.csv\"\n",
    "\n",
    "if not farm_survey_1997_file.exists():\n",
    "    parameters = (\n",
    "        \"source_desc=SURVEY\"\n",
    "        + \"&\"\n",
    "        + urllib.parse.quote(\"sector_desc=FARMS & LANDS & ASSETS\")\n",
    "        + \"&\"\n",
    "        + urllib.parse.quote(\"commodity_desc=FARM OPERATIONS\")\n",
    "        + \"&\"\n",
    "        + urllib.parse.quote(\"statisticcat_desc=AREA OPERATED\")\n",
    "        + \"&unit_desc=ACRES\"\n",
    "        + \"&freq_desc=ANNUAL\"\n",
    "        + \"&reference_period_desc=YEAR\"\n",
    "        + \"&year__GE=1997\"\n",
    "        + \"&agg_level_desc=NATIONAL\"\n",
    "        + \"&\"\n",
    "        + urllib.parse.quote(\"state_name=US TOTAL\")\n",
    "        + \"&format=CSV\"\n",
    "    )\n",
    "\n",
    "    stats = c_usda_quick_stats(MY_NASS_API_key)\n",
    "    s_json = stats.get_data(parameters, farm_survey_1997_file)\n",
    "else:\n",
    "    print(\"Skipping national_farm_survey_acres_ge_1997\")\n",
    "\n",
    "soybean_yield_data = output_dir / \"corn_yield_data_raw.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e27ac33-67a5-4329-a96d-d053779d1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not soybean_yield_data.exists():\n",
    "    parameters = (\n",
    "        \"source_desc=SURVEY\"\n",
    "        + \"&sector_desc=CROPS\"\n",
    "        + \"&\"\n",
    "        + urllib.parse.quote(\"group_desc=FIELD CROPS\")\n",
    "        + \"&commodity_desc=CORN\"\n",
    "        + \"&statisticcat_desc=YIELD\"\n",
    "        + \"&geographic_level=STATE\"\n",
    "        + \"&agg_level_desc=COUNTY\"\n",
    "        + \"&state_name=ILLINOIS\"\n",
    "        + \"&state_name=IOWA\"\n",
    "        + \"&state_name=MINNESTOTA\"\n",
    "        + \"&state_name=INDIANA\"\n",
    "        + \"&state_name=OHIO\"\n",
    "        + \"&state_name=NEBRASKA\"\n",
    "        + \"&state_name=MISSOURI\"\n",
    "        + \"&state_name=KANSAS\"\n",
    "        + \"&state_name=SOUTH_DAKOTA\"\n",
    "        + \"&state_name=COLORADO\"\n",
    "        + \"&year__GE=2003\"\n",
    "        + \"&year__LE=2022\"\n",
    "        + \"&format=CSV\"\n",
    "    )\n",
    "\n",
    "    stats = c_usda_quick_stats(MY_NASS_API_key)\n",
    "    stats.get_data(parameters, soybean_yield_data)\n",
    "else:\n",
    "    print(\"skipping soybean_yield_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247da3ca-c884-4288-b202-c0ba0a1d750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              short_desc\n",
      "0             CORN, GRAIN - YIELD, MEASURED IN BU / ACRE\n",
      "12675      CORN, SILAGE - YIELD, MEASURED IN TONS / ACRE\n",
      "14578  CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU...\n",
      "16624  CORN, SILAGE, IRRIGATED - YIELD, MEASURED IN T...\n",
      "17035  CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED I...\n",
      "19032  CORN, SILAGE, NON-IRRIGATED - YIELD, MEASURED ...\n",
      "12675\n",
      "12178\n",
      "711\n"
     ]
    }
   ],
   "source": [
    "tgt_file = archive_dir / \"corn_yield_data.csv\"\n",
    "if True or not tgt_file.exists():\n",
    "    df = pd.read_csv(soybean_yield_data)\n",
    "\n",
    "    df1 = df[[\"short_desc\"]].drop_duplicates()\n",
    "    print(df1.head(10))\n",
    "\n",
    "    df = df[df[\"short_desc\"] == \"CORN, GRAIN - YIELD, MEASURED IN BU / ACRE\"]\n",
    "    print(len(df))\n",
    "\n",
    "    bad_county_names = [\"OTHER COUNTIES\", \"OTHER (COMBINED) COUNTIES\"]\n",
    "    df = df[~df.county_name.isin(bad_county_names)]\n",
    "\n",
    "    print(len(df))\n",
    "\n",
    "    df2 = df[[\"state_name\", \"county_name\"]].drop_duplicates()\n",
    "    print(len(df2))\n",
    "\n",
    "    df = df.rename(columns={\"Value\": \"yield\"})\n",
    "\n",
    "    output_file = output_dir / \"repaired_yield_corn.csv\"\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    shutil.copyfile(output_file, tgt_file)\n",
    "else:\n",
    "    print(\"not copying \", tgt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccf8b9ab-56fc-483b-ad94-ff7dfcfdf608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state_name county_name  yield\n",
      "0  2022   COLORADO     LARIMER  191.1\n",
      "1  2020   COLORADO     LARIMER  128.2\n",
      "2  2019   COLORADO     LARIMER  148.5\n",
      "3  2016   COLORADO     LARIMER  101.2\n",
      "4  2015   COLORADO     LARIMER  130.0\n",
      "10015\n",
      "Empty DataFrame\n",
      "Columns: [year, state_name, county_name, yield]\n",
      "Index: []\n",
      "\n",
      "wrote file  ML-ARCHIVES--v01/year_state_county_yield_corn.csv\n"
     ]
    }
   ],
   "source": [
    "tgt_file_01 = archive_dir / \"year_state_county_yield_corn.csv\"\n",
    "if not tgt_file_01.exists():\n",
    "    tgt_file = archive_dir / \"corn_yield_data.csv\"\n",
    "\n",
    "    df = pd.read_csv(tgt_file).set_index([\"state_name\", \"county_name\"])\n",
    "    top_counties = (\n",
    "        df.groupby([\"state_name\", \"county_name\"])\n",
    "        .agg(\"yield\")\n",
    "        .sum()\n",
    "        .sort_values(ascending=False).head(530).reset_index().set_index([\"state_name\", \"county_name\"])\n",
    "    )\n",
    "\n",
    "    df = df[\n",
    "        df.index.isin(top_counties.index)\n",
    "    ].reset_index()\n",
    "    cols_to_keep = [\"year\", \"state_name\", \"county_name\", \"yield\"]\n",
    "    dfml = df[cols_to_keep]\n",
    "\n",
    "    print(dfml.head())\n",
    "\n",
    "    print(dfml.shape[0])\n",
    "\n",
    "    print(dfml[dfml[\"yield\"].isnull()].head())\n",
    "\n",
    "    dfml.to_csv(tgt_file_01, index=False)\n",
    "    print(\"\\nwrote file \", tgt_file_01)\n",
    "else:\n",
    "    print(\"not writing \", tgt_file_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7de25a-86c3-4c65-a30b-243df1e23bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping national_farm_survey_acres_ge_1997\n",
      "skipping soybean_yield_data\n",
      "not copying  ML-ARCHIVES--v01/corn_yield_data.csv\n",
      "not writing  ML-ARCHIVES--v01/year_state_county_yield_corn_200.csv\n",
      "not writing  ML-ARCHIVES--v01/state_county_lon_lat.csv\n",
      "Skipping state_county_lon_lat_soil\n",
      "[279, 280, 281, 282, 283]\n",
      "DUPAGE\n",
      "DUPAGE\n",
      "DUPAGE\n",
      "DUPAGE\n",
      "DUPAGE\n",
      "9952 559\n",
      "9952\n",
      "     year state_name  county_name  yield        lon        lat\n",
      "0    2022   ILLINOIS       BUREAU   67.5 -89.534118  41.401629\n",
      "20   2022   ILLINOIS      CARROLL   68.9 -89.955679  42.064735\n",
      "40   2022   ILLINOIS        HENRY   66.8 -90.117744  41.341855\n",
      "60   2022   ILLINOIS   JO DAVIESS   62.6 -90.174374  42.350666\n",
      "79   2022   ILLINOIS          LEE   66.8 -89.286030  41.747311\n",
      "99   2022   ILLINOIS       MERCER   65.0 -90.739872  41.201973\n",
      "118  2022   ILLINOIS         OGLE   67.6 -89.313860  42.039701\n",
      "138  2022   ILLINOIS       PUTNAM   64.6 -89.267641  41.202591\n",
      "156  2022   ILLINOIS  ROCK ISLAND   66.3 -90.576614  41.441179\n",
      "175  2022   ILLINOIS   STEPHENSON   62.3 -89.673564  42.350347\n",
      "  state_name  county_name        lon        lat\n",
      "0   ILLINOIS       BUREAU -89.534118  41.401629\n",
      "1   ILLINOIS      CARROLL -89.955679  42.064735\n",
      "2   ILLINOIS        HENRY -90.117744  41.341855\n",
      "3   ILLINOIS   JO DAVIESS -90.174374  42.350666\n",
      "4   ILLINOIS          LEE -89.286030  41.747311\n",
      "5   ILLINOIS       MERCER -90.739872  41.201973\n",
      "6   ILLINOIS         OGLE -89.313860  42.039701\n",
      "7   ILLINOIS       PUTNAM -89.267641  41.202591\n",
      "8   ILLINOIS  ROCK ISLAND -90.576614  41.441179\n",
      "9   ILLINOIS   STEPHENSON -89.673564  42.350347\n",
      "     year state_name county_name  yield        lon        lat\n",
      "279  2007   ILLINOIS      DUPAGE   48.0 -88.090687  41.860374\n",
      "280  2006   ILLINOIS      DUPAGE   47.0 -88.090687  41.860374\n",
      "281  2005   ILLINOIS      DUPAGE   33.0 -88.090687  41.860374\n",
      "282  2004   ILLINOIS      DUPAGE   42.0 -88.090687  41.860374\n",
      "283  2003   ILLINOIS      DUPAGE   36.0 -88.090687  41.860374\n",
      "wrote file:  ML-ARCHIVES--v01/year_state_county_yield_lon_lat.csv\n",
      "214\n",
      "          T2M_MAX  T2M_MIN  PRECTOTCORR  GWETROOT  EVPTRNS  ALLSKY_SFC_PAR_TOT\n",
      "20220401     7.94    -5.22         0.05      0.74     0.06              116.01\n",
      "20220402     7.16    -1.35         4.71      0.74     0.00               35.17\n",
      "20220403    11.33    -3.40         2.21      0.74     0.10               97.62\n",
      "20220404    11.17     3.71         2.17      0.74     0.09               46.62\n",
      "20220405    12.72    -1.72         4.39      0.74     0.02               63.50\n",
      "start and end times were:  2023-06- 09:47:13 2023-06- 09:47:14\n",
      "(9952, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hb/kc7s7llx2tbc5b8gtqnc1n8w0000gn/T/ipykernel_73087/2889926549.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0mw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_yscyll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwdtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0mw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mdtype_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                     \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m                 \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    705\u001b[0m             )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;31m# GH10856\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleArrayManager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Input is now list-like, so rely on \"standard\" construction:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# Now we just make sure the order is respected, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleArrayManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6000\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6002\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6003\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6004\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6006\u001b[0m         \u001b[0;31m# if this fails, go on to more involved attribute setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;36m2\u001b[0m            \u001b[0;36m5\u001b[0m             \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Even Numbers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;34m'Even Numbers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \"\"\"\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[0;32m-> 5975\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5976\u001b[0m         \"\"\"\n\u001b[1;32m   5977\u001b[0m         \u001b[0mAfter\u001b[0m \u001b[0mregular\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0maccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mtry\u001b[0m \u001b[0mlooking\u001b[0m \u001b[0mup\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5978\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mallows\u001b[0m \u001b[0msimpler\u001b[0m \u001b[0maccess\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minteractive\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_county_lon_lat = archive_dir / \"state_county_lon_lat.csv\"\n",
    "if not state_county_lon_lat.exists():\n",
    "    df = pd.read_csv(tgt_file_01)\n",
    "    print(\"number of rows in csv cleaned for ML: \", len(df))\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    df1 = df[[\"state_name\", \"county_name\"]].drop_duplicates()\n",
    "    print(\"\\nNumber of state-county pairs is: \", len(df1))\n",
    "\n",
    "    index = df.index[\n",
    "        (df[\"county_name\"] == \"DU PAGE\") | (df[\"county_name\"] == \"DUPAGE\")\n",
    "    ].tolist()\n",
    "    for ind in index:\n",
    "        df.at[ind, \"county_name\"] = \"DUPAGE\"\n",
    "\n",
    "    index1 = df1.index[\n",
    "        (df1[\"county_name\"] == \"DU PAGE\") | (df1[\"county_name\"] == \"DUPAGE\")\n",
    "    ].tolist()\n",
    "    for ind in index1:\n",
    "        df1.at[ind, \"county_name\"] = \"DUPAGE\"\n",
    "\n",
    "    def geocode_county(state, county):\n",
    "        geolocator = Nominatim(user_agent=\"county_geocoder\")\n",
    "        location = geolocator.geocode(county + \", \" + state + \", USA\")\n",
    "        sleep(0.5)\n",
    "        if location:\n",
    "            return location.longitude, location.latitude\n",
    "        else:\n",
    "            print(\"no lat-lon found for \", state, county)\n",
    "            return None, None\n",
    "\n",
    "    df1[[\"lon\", \"lat\"]] = df1.apply(\n",
    "        lambda x: geocode_county(x[\"state_name\"], x[\"county_name\"]),\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "\n",
    "    print(df1.head())\n",
    "\n",
    "    print(\"lon-lat for ILLINOIS-BUREAU is: \", geocode_county(\"ILLINOIS\", \"BUREAU\"))\n",
    "\n",
    "    df1.to_csv(state_county_lon_lat, index=False)\n",
    "    print(\"wrote file: \", state_county_lon_lat)\n",
    "else:\n",
    "    print(\"not writing \", state_county_lon_lat)\n",
    "\n",
    "\n",
    "state_county_lon_lat_soil = archive_dir / \"state_county_lon_lat_soil.csv\"\n",
    "if not state_county_lon_lat_soil.exists():\n",
    "    scll_filename = archive_dir / \"state_county_lon_lat.csv\"\n",
    "    df_scll = pd.read_csv(scll_filename)\n",
    "    print(df_scll.head())\n",
    "    print(len(df_scll))\n",
    "    urlkeys = {\n",
    "        \"AEZ_classes\": \"https://s3.eu-west-1.amazonaws.com/data.gaezdev.aws.fao.org/LR/aez/aez_v9v2red_5m_CRUTS32_Hist_8110_100_avg.tif\",\n",
    "        \"nutr_ret_high\": \"https://s3.eu-west-1.amazonaws.com/data.gaezdev.aws.fao.org/LR/soi1/SQ2_mze_v9aH.tif\",\n",
    "        \"soil_qual_high\": \"https://s3.eu-west-1.amazonaws.com/data.gaezdev.aws.fao.org/LR/soi1/SQ0_mze_v9aH.tif\",\n",
    "        \"soil_qual_low\": \"https://s3.eu-west-1.amazonaws.com/data.gaezdev.aws.fao.org/LR/soi1/SQ0_mze_v9aL.tif\",\n",
    "        \"suit_irrig_high_soy\": \"https://s3.eu-west-1.amazonaws.com/data.gaezdev.aws.fao.org/res05/CRUTS32/Hist/8110H/suHi_soy.tif\",\n",
    "    }\n",
    "\n",
    "    fileFullName = {}\n",
    "\n",
    "    for key, url in urlkeys.items():\n",
    "        tif_file = tif_dir / f\"{key}.tif\"\n",
    "        if not tif_file.exists():\n",
    "            fileFullName[key] = tif_file\n",
    "            print(fileFullName[key])\n",
    "            urllib.request.urlretrieve(url, tif_file)\n",
    "        else:\n",
    "            print(\"not retrieving \", key)\n",
    "\n",
    "    def pull_useful(\n",
    "        ginfo,\n",
    "    ):\n",
    "        useful = {}\n",
    "        useful[\"band_count\"] = len(ginfo[\"bands\"])\n",
    "\n",
    "        useful[\"size\"] = ginfo[\"size\"]\n",
    "\n",
    "        return useful\n",
    "\n",
    "    gdalInfoReq = {}\n",
    "    gdalInfo = {}\n",
    "    useful = {}\n",
    "    for k in urlkeys.keys():\n",
    "        gdalInfoReq[k] = \" \".join([\"gdalinfo\", \"-json\", fileFullName[k]])\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [gdalInfoReq[k]], shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        gdalInfo[k] = json.loads(result.stdout)\n",
    "\n",
    "        useful[k] = pull_useful(gdalInfo[k])\n",
    "        print(\"\\n\", k)\n",
    "        print(json.dumps(useful[k], indent=2, sort_keys=True))\n",
    "\n",
    "    tiff_file = fileFullName[\"AEZ_classes\"]\n",
    "\n",
    "    print(df_scll.iloc[[0]])\n",
    "    test_lon = df_scll.iloc[0][\"lon\"]\n",
    "    test_lat = df_scll.iloc[0][\"lat\"]\n",
    "    print(test_lon, test_lat, type(test_lon), type(test_lat))\n",
    "    val = get_coordinate_pixel(tiff_file, test_lon, test_lat)\n",
    "    print(type(val))\n",
    "    print(val)\n",
    "\n",
    "    df3 = df_scll.copy()\n",
    "    print(df3.head())\n",
    "    print(len(df3))\n",
    "    for k in urlkeys.keys():\n",
    "        tiff_file = fileFullName[k]\n",
    "        df3[k] = df3.apply(\n",
    "            lambda r: get_coordinate_pixel(tiff_file, r[\"lon\"], r[\"lat\"]), axis=1\n",
    "        )\n",
    "    print(df3.head())\n",
    "    print(len(df3))\n",
    "\n",
    "    for k in urlkeys.keys():\n",
    "        print(k)\n",
    "        print(df3[[k]].drop_duplicates().head(100))\n",
    "\n",
    "    df4 = df3.copy()\n",
    "\n",
    "    one_hot = pd.get_dummies(df4[\"AEZ_classes\"])\n",
    "\n",
    "    df4 = df4.drop(\"AEZ_classes\", axis=1)\n",
    "\n",
    "    df4 = df4.join(one_hot)\n",
    "    print(len(df4))\n",
    "    print(df4.head())\n",
    "    print(df4.columns.tolist())\n",
    "\n",
    "    cols = {\n",
    "        16: \"AEZ_1\",\n",
    "        17: \"AEZ_2\",\n",
    "        18: \"AEZ_3\",\n",
    "        19: \"AEZ_4\",\n",
    "        20: \"AEZ_5\",\n",
    "        21: \"AEZ_6\",\n",
    "        27: \"AEZ_7\",\n",
    "        28: \"AEZ_8\",\n",
    "        32: \"AEZ_9\",\n",
    "    }\n",
    "    df4 = df4.rename(columns=cols)\n",
    "    print(df4.columns.tolist())\n",
    "    print(df4.head())\n",
    "\n",
    "    df5 = df4.copy()\n",
    "\n",
    "    one_hot1 = pd.get_dummies(df5[\"soil_qual_high\"])\n",
    "\n",
    "    df5 = df5.drop(\"soil_qual_high\", axis=1)\n",
    "\n",
    "    df5 = df5.join(one_hot1)\n",
    "    print(len(df5))\n",
    "    print(df5.head())\n",
    "    print(df5.columns.tolist())\n",
    "\n",
    "    cols = {\n",
    "        4: \"SQH_1\",\n",
    "        5: \"SQH_2\",\n",
    "        6: \"SQH_3\",\n",
    "        7: \"SQH_4\",\n",
    "        8: \"SQH_5\",\n",
    "        9: \"SQH_6\",\n",
    "        10: \"SQH_7\",\n",
    "    }\n",
    "    df5 = df5.rename(columns=cols)\n",
    "    print(df5.columns.tolist())\n",
    "    print(df5.head())\n",
    "\n",
    "    df6 = df5.copy()\n",
    "\n",
    "    one_hot2 = pd.get_dummies(df6[\"soil_qual_low\"])\n",
    "\n",
    "    df6 = df6.drop(\"soil_qual_low\", axis=1)\n",
    "\n",
    "    df6 = df6.join(one_hot2)\n",
    "    print(len(df6))\n",
    "    print(df6.head())\n",
    "    print(df6.columns.tolist())\n",
    "\n",
    "    cols = {\n",
    "        4: \"SQL_1\",\n",
    "        5: \"SQL_2\",\n",
    "        6: \"SQL_3\",\n",
    "        7: \"SQL_4\",\n",
    "        8: \"SQL_5\",\n",
    "        9: \"SQL_6\",\n",
    "        10: \"SQL_7\",\n",
    "    }\n",
    "    df6 = df6.rename(columns=cols)\n",
    "    print(df6.columns.tolist())\n",
    "    print(df6.head())\n",
    "    state_county_lon_lat_soil = archive_dir / \"state_county_lon_lat_soil.csv\"\n",
    "    df6.to_csv(state_county_lon_lat_soil, index=False)\n",
    "    print(\"wrote file: \", state_county_lon_lat_soil)\n",
    "else:\n",
    "    print(\"Skipping state_county_lon_lat_soil\")\n",
    "\n",
    "\n",
    "yscy_file = archive_dir / \"year_state_county_yield.csv\"\n",
    "scll_file = archive_dir / \"state_county_lon_lat.csv\"\n",
    "df_yscy = pd.read_csv(yscy_file)\n",
    "df_scll = pd.read_csv(scll_file)\n",
    "\n",
    "\n",
    "index_list = df_yscy.index[\n",
    "    (df_yscy[\"county_name\"] == \"DU PAGE\") | (df_yscy[\"county_name\"] == \"DUPAGE\")\n",
    "].tolist()\n",
    "print(index_list)\n",
    "for i in index_list:\n",
    "    df_yscy.at[i, \"county_name\"] = \"DUPAGE\"\n",
    "    print(df_yscy.at[i, \"county_name\"])\n",
    "print(len(df_yscy), len(df_scll))\n",
    "df_yscyll = pd.merge(df_yscy, df_scll, on=[\"state_name\", \"county_name\"], how=\"left\")\n",
    "print(len(df_yscyll))\n",
    "\n",
    "\n",
    "print(df_yscyll[df_yscyll[\"year\"] == 2022].head(10))\n",
    "print(df_scll.head(10))\n",
    "\n",
    "print(df_yscyll.iloc[279:284].head())\n",
    "yscyll_filename = archive_dir / \"year_state_county_yield_lon_lat.csv\"\n",
    "df_yscyll.to_csv(yscyll_filename, index=False)\n",
    "print(\"wrote file: \", yscyll_filename)\n",
    "yscyll_filename = archive_dir / \"year_state_county_yield_lon_lat.csv\"\n",
    "df_yscyll = pd.read_csv(yscyll_filename)\n",
    "\n",
    "weather_params = [\n",
    "    \"T2M_MAX\",\n",
    "    \"T2M_MIN\",\n",
    "    \"PRECTOTCORR\",\n",
    "    \"GWETROOT\",\n",
    "    \"EVPTRNS\",\n",
    "    \"ALLSKY_SFC_PAR_TOT\",\n",
    "]\n",
    "\"\"\"\n",
    "   T2M_MAX: The maximum hourly air (dry bulb) temperature at 2 meters above the surface of the\n",
    "             earth in the period of interest.\n",
    "   T2M_MIN: The minimum hourly air (dry bulb) temperature at 2 meters above the surface of the\n",
    "            earth in the period of interest.\n",
    "   PRECTOTCORR: The bias corrected average of total precipitation at the surface of the earth\n",
    "                in water mass (includes water content in snow)\n",
    "   EVPTRNS: The evapotranspiration energy flux at the surface of the earth\n",
    "   ALLSKY_SFC_PAR_TOT: The total Photosynthetically Active Radiation (PAR) incident\n",
    "         on a horizontal plane at the surface of the earth under all sky conditions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "base_url = r\"https://power.larc.nasa.gov/api/temporal/daily/point?\"\n",
    "base_url += (\n",
    "    \"parameters=T2M_MAX,T2M_MIN,PRECTOTCORR,GWETROOT,EVPTRNS,ALLSKY_SFC_PAR_TOT&\"\n",
    ")\n",
    "base_url += \"community=RE&longitude={longitude}&latitude={latitude}&start={year}0401&end={year}1031&format=JSON\"\n",
    "\n",
    "\n",
    "def fetch_weather_county_year(year, state, county):\n",
    "    row = df_yscyll.loc[\n",
    "        (df_yscyll[\"state_name\"] == state)\n",
    "        & (df_yscyll[\"county_name\"] == county)\n",
    "        & (df_yscyll[\"year\"] == year)\n",
    "    ]\n",
    "\n",
    "    lon = row.iloc[0][\"lon\"]\n",
    "    lat = row.iloc[0][\"lat\"]\n",
    "\n",
    "    api_request_url = base_url.format(longitude=lon, latitude=lat, year=str(year))\n",
    "\n",
    "    response = requests.get(url=api_request_url, verify=True, timeout=30.00)\n",
    "\n",
    "    content = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    weather = content[\"properties\"][\"parameter\"]\n",
    "\n",
    "    df = pd.DataFrame(weather)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = fetch_weather_county_year(2022, \"ILLINOIS\", \"LEE\")\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "w_df = {}\n",
    "\n",
    "\n",
    "out_dir = archive_dir / \"WEATHER-DATA--v01/\"\n",
    "filename = r\"weather-data-for-index__{index}.csv\"\n",
    "\n",
    "starttime = datetime.datetime.now().strftime(\"%Y-%m-% %H:%M:%S\")\n",
    "\n",
    "for i in range(0, len(df_yscyll)):\n",
    "    row = df_yscyll.iloc[i]\n",
    "    outfilename = out_dir / filename.format(index=str(i).zfill(4))\n",
    "\n",
    "    if Path(outfilename).exists():\n",
    "        continue\n",
    "\n",
    "    w_df[i] = fetch_weather_county_year(\n",
    "        row[\"year\"], row[\"state_name\"], row[\"county_name\"]\n",
    "    )\n",
    "    w_df[i].to_csv(outfilename)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\n",
    "            \"\\nFinished work on index: \",\n",
    "            i,\n",
    "            \"     at time: \",\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-% %H:%M:%S\"),\n",
    "        )\n",
    "        print(\"   This involved fetching weather data for the following row:\")\n",
    "        print(\n",
    "            row[\"year\"], row[\"state_name\"], row[\"county_name\"], row[\"lon\"], row[\"lat\"]\n",
    "        )\n",
    "        print(\"Wrote file: \", outfilename)\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now().strftime(\"%Y-%m-% %H:%M:%S\")\n",
    "print(\"start and end times were: \", starttime, endtime)\n",
    "\n",
    "yscyll_filename = \"year_state_county_yield_lon_lat.csv\"\n",
    "\n",
    "wdtemplate = r\"weather-data-for-index__{padded}.csv\"\n",
    "\n",
    "df_yscyll = pd.read_csv(archive_dir / yscyll_filename)\n",
    "print(df_yscyll.shape)\n",
    "\n",
    "\n",
    "w_df = {}\n",
    "for i in range(0, len(df_yscyll)):\n",
    "    padded = str(i).zfill(4)\n",
    "    w_df[i] = pd.read_csv(weather_dir / wdtemplate.format(padded=padded))\n",
    "    w_df[i].rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(w_df[4].shape)\n",
    "print(w_df[4].head())\n",
    "\n",
    "\n",
    "def create_monthly_df(df):\n",
    "    df1 = df.copy()\n",
    "\n",
    "    df1.index = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "    df1_monthly = df1.resample(\"M\").agg(\n",
    "        {\n",
    "            \"T2M_MAX\": \"mean\",\n",
    "            \"T2M_MIN\": \"mean\",\n",
    "            \"PRECTOTCORR\": \"sum\",\n",
    "            \"GWETROOT\": \"mean\",\n",
    "            \"EVPTRNS\": \"mean\",\n",
    "            \"ALLSKY_SFC_PAR_TOT\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df1_monthly.index = df1_monthly.index.strftime(\"%Y%m%d\")\n",
    "\n",
    "    return df1_monthly\n",
    "\n",
    "\n",
    "print(create_monthly_df(w_df[4]).head(50))\n",
    "\n",
    "\n",
    "df_t0 = w_df[0]\n",
    "cols_narrow = df_t0.columns.values.tolist()[1:]\n",
    "print(cols_narrow)\n",
    "\n",
    "\n",
    "df_t1 = create_monthly_df(df_t0)\n",
    "print(len(df_t1))\n",
    "\n",
    "\n",
    "cols_wide = []\n",
    "for i in range(0, len(df_t1)):\n",
    "    row = df_t1.iloc[i]\n",
    "\n",
    "    month_id = \"month_\" + str(i).zfill(2)\n",
    "\n",
    "    for c in cols_narrow:\n",
    "        cols_wide.append(month_id + \"__\" + c)\n",
    "\n",
    "print(cols_wide)\n",
    "print(len(cols_wide))\n",
    "\n",
    "\n",
    "print(w_df[0].columns.tolist()[1:])\n",
    "print(w_df[0].shape)\n",
    "print(create_monthly_df(w_df[0]).shape)\n",
    "\n",
    "\n",
    "def create_weather_seq_for_monthly(dfw):\n",
    "    seq = []\n",
    "    cols = dfw.columns.tolist()\n",
    "    for i in range(0, len(dfw)):\n",
    "        for c in cols:\n",
    "            seq.append(dfw.iloc[i][c])\n",
    "    return seq\n",
    "\n",
    "\n",
    "dfw = create_monthly_df(w_df[0])\n",
    "print(dfw.head(10))\n",
    "\n",
    "seqw = create_weather_seq_for_monthly(dfw)\n",
    "print(json.dumps(seqw, indent=4))\n",
    "\n",
    "\n",
    "u_df = {}\n",
    "dfw = {}\n",
    "seqw = {}\n",
    "\n",
    "\n",
    "for i in range(0, len(df_yscyll)):\n",
    "    padded = str(i).zfill(4)\n",
    "\n",
    "    u_df[padded] = pd.read_csv(weather_dir / wdtemplate.format(padded=padded))\n",
    "\n",
    "    u_df[padded].rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "\n",
    "    dfw[padded] = create_monthly_df(u_df[padded])\n",
    "\n",
    "    seqw[i] = create_weather_seq_for_monthly(dfw[padded])\n",
    "\n",
    "\n",
    "print(len(seqw))\n",
    "print(json.dumps(seqw, indent=4))\n",
    "\n",
    "\n",
    "print(dfw[\"0000\"].shape)\n",
    "print(len(cols_wide))\n",
    "print(len(df_yscyll))\n",
    "print(len(seqw[0]))\n",
    "\n",
    "\n",
    "df_wide_weather_monthly = pd.DataFrame.from_dict(\n",
    "    seqw, orient=\"index\", columns=cols_wide\n",
    ")\n",
    "\n",
    "print(df_wide_weather_monthly.shape)\n",
    "\n",
    "print(df_wide_weather_monthly.head())\n",
    "\n",
    "\n",
    "sclls_file = archive_dir / \"state_county_lon_lat_soil.csv\"\n",
    "\n",
    "df_scsoil = pd.read_csv(sclls_file).drop(columns=[\"lon\", \"lat\"])\n",
    "print(df_scsoil.shape)\n",
    "\n",
    "\n",
    "df_ysc_y_soil = pd.merge(\n",
    "    df_yscyll, df_scsoil, on=[\"state_name\", \"county_name\"], how=\"left\"\n",
    ")\n",
    "\n",
    "df_ysc_y_soil = df_ysc_y_soil.drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "\n",
    "print(df_ysc_y_soil.shape)\n",
    "print(df_ysc_y_soil.head())\n",
    "\n",
    "\n",
    "df_ysc_y_soil_weather_monthly = pd.concat(\n",
    "    [df_ysc_y_soil, df_wide_weather_monthly], axis=\"columns\"\n",
    ")\n",
    "\n",
    "print(df_ysc_y_soil_weather_monthly.shape)\n",
    "\n",
    "print(df_ysc_y_soil_weather_monthly.loc[28:32, :])\n",
    "\n",
    "\n",
    "ml_file = ml_tables_dir / \"ML-table-monthly.csv\"\n",
    "\n",
    "df_ysc_y_soil_weather_monthly.to_csv(ml_file, index=False)\n",
    "\n",
    "print(\"Wrote file \", ml_file)\n",
    "\n",
    "\n",
    "yscyll_filename = archive_dir / \"year_state_county_yield_lon_lat.csv\"\n",
    "\n",
    "wdtemplate = r\"weather-data-for-index__{padded}.csv\"\n",
    "\n",
    "df_yscyll = pd.read_csv(yscyll_filename)\n",
    "print(df_yscyll.shape)\n",
    "\n",
    "\n",
    "w_df = {}\n",
    "for i in range(0, len(df_yscyll)):\n",
    "    padded = str(i).zfill(4)\n",
    "    w_df[i] = pd.read_csv(weather_dir / wdtemplate.format(padded=padded))\n",
    "\n",
    "    w_df[i].rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(w_df[4].shape)\n",
    "print(w_df[4].head())\n",
    "\n",
    "\n",
    "def create_weekly_df(df):\n",
    "    df1 = df.copy()\n",
    "\n",
    "    df1.index = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "    df1_weekly = df1.resample(\"W\").agg(\n",
    "        {\n",
    "            \"T2M_MAX\": \"mean\",\n",
    "            \"T2M_MIN\": \"mean\",\n",
    "            \"PRECTOTCORR\": \"sum\",\n",
    "            \"GWETROOT\": \"mean\",\n",
    "            \"EVPTRNS\": \"mean\",\n",
    "            \"ALLSKY_SFC_PAR_TOT\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df1_weekly.index = df1_weekly.index.strftime(\"%Y%m%d\")\n",
    "\n",
    "    return df1_weekly\n",
    "\n",
    "\n",
    "print(create_weekly_df(w_df[4]).head(50))\n",
    "\n",
    "\n",
    "df_t0 = w_df[0]\n",
    "cols_narrow = df_t0.columns.values.tolist()[1:]\n",
    "print(cols_narrow)\n",
    "\n",
    "\n",
    "df_t1 = create_weekly_df(df_t0)\n",
    "\n",
    "\n",
    "cols_wide = []\n",
    "for i in range(0, len(df_t1)):\n",
    "    row = df_t1.iloc[i]\n",
    "\n",
    "    week_id = \"week_\" + str(i).zfill(2)\n",
    "\n",
    "    for c in cols_narrow:\n",
    "        cols_wide.append(week_id + \"__\" + c)\n",
    "\n",
    "print(cols_wide)\n",
    "\n",
    "\n",
    "print(w_df[0].columns.tolist()[1:])\n",
    "print(w_df[0].shape)\n",
    "print(create_weekly_df(w_df[0]).shape)\n",
    "\n",
    "\n",
    "def create_weather_seq_for_weekly(dfw):\n",
    "    seq = []\n",
    "    for i in range(0, len(dfw)):\n",
    "        cols = dfw.columns.tolist()\n",
    "        for c in cols:\n",
    "            seq.append(dfw.iloc[i][c])\n",
    "    return seq\n",
    "\n",
    "\n",
    "dfw = create_weekly_df(w_df[0])\n",
    "print(dfw.head(10))\n",
    "\n",
    "seqw = create_weather_seq_for_weekly(dfw)\n",
    "print(json.dumps(seqw, indent=4))\n",
    "\n",
    "\n",
    "u_df = {}\n",
    "dfw = {}\n",
    "seqw = {}\n",
    "\n",
    "\n",
    "for i in range(0, len(df_yscyll)):\n",
    "    padded = str(i).zfill(4)\n",
    "\n",
    "    u_df[padded] = pd.read_csv(weather_dir / wdtemplate.format(padded=padded))\n",
    "\n",
    "    u_df[padded].rename(columns={\"Unnamed: 0\": \"date\"}, inplace=True)\n",
    "\n",
    "    dfw[padded] = create_weekly_df(u_df[padded])\n",
    "\n",
    "    seqw[i] = create_weather_seq_for_weekly(dfw[padded])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Completed processing of index \", i)\n",
    "\n",
    "\n",
    "print(print(json.dumps(seqw, indent=4)))\n",
    "\n",
    "\n",
    "print(len(seqw))\n",
    "\n",
    "\n",
    "print(dfw[\"0000\"].shape)\n",
    "print(len(cols_wide))\n",
    "print(len(df_yscyll))\n",
    "print(len(seqw[0]))\n",
    "\n",
    "\n",
    "df_wide_weather_weekly_prelim = pd.DataFrame.from_dict(\n",
    "    seqw, orient=\"index\", columns=cols_wide\n",
    ")\n",
    "\n",
    "print(df_wide_weather_weekly_prelim.shape)\n",
    "\n",
    "print(df_wide_weather_weekly_prelim.head())\n",
    "\n",
    "\n",
    "print(df_wide_weather_weekly_prelim.shape)\n",
    "week_31_cols = [\n",
    "    \"week_31__T2M_MAX\",\n",
    "    \"week_31__T2M_MIN\",\n",
    "    \"week_31__PRECTOTCORR\",\n",
    "    \"week_31__GWETROOT\",\n",
    "    \"week_31__EVPTRNS\",\n",
    "    \"week_31__ALLSKY_SFC_PAR_TOT\",\n",
    "]\n",
    "\n",
    "df_wide_weather_weekly = df_wide_weather_weekly_prelim.drop(columns=week_31_cols)\n",
    "\n",
    "\n",
    "print(df_wide_weather_weekly.shape)\n",
    "print(df_wide_weather_weekly.head())\n",
    "\n",
    "\n",
    "sclls_file = archive_dir / \"state_county_lon_lat_soil.csv\"\n",
    "\n",
    "df_scsoil = pd.read_csv(sclls_file).drop(columns=[\"lon\", \"lat\"])\n",
    "print(df_scsoil.shape)\n",
    "\n",
    "\n",
    "df_ysc_y_soil = pd.merge(\n",
    "    df_yscyll, df_scsoil, on=[\"state_name\", \"county_name\"], how=\"left\"\n",
    ")\n",
    "\n",
    "df_ysc_y_soil = df_ysc_y_soil.drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "\n",
    "print(df_ysc_y_soil.shape)\n",
    "print(df_ysc_y_soil.head())\n",
    "\n",
    "\n",
    "df_ysc_y_soil_weather_weekly = pd.concat(\n",
    "    [df_ysc_y_soil, df_wide_weather_weekly], axis=\"columns\"\n",
    ")\n",
    "\n",
    "print(df_ysc_y_soil_weather_weekly.shape)\n",
    "\n",
    "print(df_ysc_y_soil_weather_weekly.loc[28:32, :])\n",
    "\n",
    "\n",
    "ml_file = ml_tables_dir / \"ML-table-weekly.csv\"\n",
    "\n",
    "df_ysc_y_soil_weather_weekly.to_csv(ml_file, index=False)\n",
    "\n",
    "print(\"Wrote file \", ml_file)\n",
    "\n",
    "\n",
    "ml_file = ml_tables_dir / \"ML-table-monthly.csv\"\n",
    "\n",
    "\n",
    "df_ml = pd.read_csv(ml_file)\n",
    "print(df_ml.shape)\n",
    "print(df_ml.head())\n",
    "\n",
    "\n",
    "print(df_ml.isnull().values.any())\n",
    "\n",
    "\n",
    "X = df_ml.drop(columns=[\"yield\"])\n",
    "y = df_ml.loc[:, [\"yield\"]]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "y_test_orig = y_test.copy()\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "print(X_test.head())\n",
    "print(y_test.head())\n",
    "\n",
    "print(y_test_orig.head())\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test.iloc[0, 0])\n",
    "\n",
    "\n",
    "X_train = X_train.drop(columns=[\"year\", \"state_name\", \"county_name\"])\n",
    "X_test = X_test.drop(columns=[\"year\", \"state_name\", \"county_name\"])\n",
    "\n",
    "\n",
    "scalerXST = StandardScaler().fit(X_train)\n",
    "scaleryST = StandardScaler().fit(y_train)\n",
    "\n",
    "X_trainST = scalerXST.transform(X_train)\n",
    "y_trainST = scaleryST.transform(y_train)\n",
    "X_testST = scalerXST.transform(X_test)\n",
    "y_testST = scaleryST.transform(y_test)\n",
    "\n",
    "\n",
    "print(df_ml.iloc[7397][\"yield\"])\n",
    "print(y_testST[0])\n",
    "print(scaleryST.inverse_transform(y_testST)[0])\n",
    "\n",
    "\n",
    "lassoST = Lasso(alpha=0.1)\n",
    "lassoST.fit(X_trainST, y_trainST)\n",
    "\n",
    "\n",
    "y_predST = lassoST.predict(X_testST)\n",
    "\n",
    "print(y_predST)\n",
    "\n",
    "\n",
    "rmseST = math.sqrt(mean_squared_error(y_testST, y_predST))\n",
    "\n",
    "print(rmseST)\n",
    "\n",
    "\n",
    "def plot_predictions(y_test, y_pred, descrip_of_run):\n",
    "    if len(y_test) != len(y_pred):\n",
    "        raise ValueError(\"The input arrays must have the same length.\")\n",
    "\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.plot(y_test, y_test, color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"y_test\")\n",
    "    plt.ylabel(\"y_pred\")\n",
    "    plt.title(\"Predicted vs Actual for \" + descrip_of_run)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_predictions(y_testST, y_predST, \"Lasso with StandardScalar\")\n",
    "\n",
    "\n",
    "linearST = LinearRegression()\n",
    "\n",
    "\n",
    "print(type(y_trainST))\n",
    "\n",
    "linearST.fit(X_trainST, y_trainST)\n",
    "\n",
    "\n",
    "y_predST = linearST.predict(X_testST)\n",
    "\n",
    "rmseST = math.sqrt(mean_squared_error(y_testST, y_predST))\n",
    "print(rmseST)\n",
    "\n",
    "plot_predictions(y_testST, y_predST, \"Linear Regression using StandardScaler\")\n",
    "\n",
    "\n",
    "scalerXMM = MinMaxScaler().fit(X_train)\n",
    "scaleryMM = MinMaxScaler().fit(y_train)\n",
    "\n",
    "X_trainMM = scalerXMM.transform(X_train)\n",
    "y_trainMM = scaleryMM.transform(y_train)\n",
    "X_testMM = scalerXMM.transform(X_test)\n",
    "y_testMM = scaleryMM.transform(y_test)\n",
    "\n",
    "\n",
    "print(df_ml.iloc[7397][\"yield\"])\n",
    "print(y_testMM[0])\n",
    "print(scaleryMM.inverse_transform(y_testMM)[0])\n",
    "\n",
    "\n",
    "linearMM = LinearRegression()\n",
    "\n",
    "linearMM.fit(X_trainMM, y_trainMM)\n",
    "\n",
    "y_predMM = linearMM.predict(X_testMM)\n",
    "\n",
    "rmseMM = math.sqrt(mean_squared_error(y_testMM, y_predMM))\n",
    "rrmseMM = rmseMM / (0.5)\n",
    "r2MM = r2_score(y_testMM, y_predMM)\n",
    "print(rmseMM)\n",
    "print(rrmseMM)\n",
    "print(r2MM)\n",
    "\n",
    "\n",
    "plot_predictions(y_testMM, y_predMM, \"Linear Regression using MinMaxScaler\")\n",
    "\n",
    "\n",
    "regrMM = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "\n",
    "regrMM.fit(X_trainMM, y_trainMM.ravel())\n",
    "\n",
    "y_predMM = regrMM.predict(X_testMM)\n",
    "rmseMM = math.sqrt(mean_squared_error(y_testMM, y_predMM))\n",
    "rrmseMM = rmseMM / (0.5)\n",
    "r2MM = r2_score(y_testMM, y_predMM)\n",
    "print(rmseMM)\n",
    "print(rrmseMM)\n",
    "print(r2MM)\n",
    "\n",
    "\n",
    "plot_predictions(y_testMM, y_predMM, \"Random Forest Regressor using MinMaxScaler\")\n",
    "\n",
    "\n",
    "ml_file = ml_tables_dir / \"ML-table-weekly.csv\"\n",
    "\n",
    "\n",
    "df_ml = pd.read_csv(ml_file)\n",
    "print(df_ml.shape)\n",
    "print(df_ml.head())\n",
    "\n",
    "\n",
    "print(df_ml.isnull().values.any())\n",
    "\n",
    "\n",
    "X = df_ml.drop(columns=[\"yield\"])\n",
    "y = df_ml.loc[:, [\"yield\"]]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "y_test_orig = y_test.copy()\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "print(X_test.head())\n",
    "print(y_test.head())\n",
    "\n",
    "print(y_test_orig.head())\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test.iloc[0, 0])\n",
    "\n",
    "\n",
    "X_train = X_train.drop(columns=[\"year\", \"state_name\", \"county_name\"])\n",
    "X_test = X_test.drop(columns=[\"year\", \"state_name\", \"county_name\"])\n",
    "\n",
    "\n",
    "scalerXST = StandardScaler().fit(X_train)\n",
    "scaleryST = StandardScaler().fit(y_train)\n",
    "\n",
    "X_trainST = scalerXST.transform(X_train)\n",
    "y_trainST = scaleryST.transform(y_train)\n",
    "X_testST = scalerXST.transform(X_test)\n",
    "y_testST = scaleryST.transform(y_test)\n",
    "\n",
    "\n",
    "print(df_ml.iloc[7397][\"yield\"])\n",
    "print(y_testST[0])\n",
    "print(scaleryST.inverse_transform(y_testST)[0])\n",
    "\n",
    "\n",
    "lassoST = Lasso(alpha=0.1)\n",
    "lassoST.fit(X_trainST, y_trainST)\n",
    "\n",
    "\n",
    "y_predST = lassoST.predict(X_testST)\n",
    "\n",
    "print(y_predST)\n",
    "\n",
    "\n",
    "rmseST = math.sqrt(mean_squared_error(y_testST, y_predST))\n",
    "\n",
    "print(rmseST)\n",
    "\n",
    "\n",
    "def plot_predictions(y_test, y_pred, descrip_of_run):\n",
    "    if len(y_test) != len(y_pred):\n",
    "        raise ValueError(\"The input arrays must have the same length.\")\n",
    "\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.plot(y_test, y_test, color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"y_test\")\n",
    "    plt.ylabel(\"y_pred\")\n",
    "    plt.title(\"Predicted vs Actual for \" + descrip_of_run)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_predictions(y_testST, y_predST, \"Lasso with StandardScalar\")\n",
    "\n",
    "\n",
    "linearST = LinearRegression()\n",
    "\n",
    "\n",
    "print(type(y_trainST))\n",
    "\n",
    "linearST.fit(X_trainST, y_trainST)\n",
    "\n",
    "\n",
    "y_predST = linearST.predict(X_testST)\n",
    "\n",
    "rmseST = math.sqrt(mean_squared_error(y_testST, y_predST))\n",
    "print(rmseST)\n",
    "\n",
    "plot_predictions(y_testST, y_predST, \"Linear Regression using StandardScaler\")\n",
    "\n",
    "\n",
    "scalerXMM = MinMaxScaler().fit(X_train)\n",
    "scaleryMM = MinMaxScaler().fit(y_train)\n",
    "\n",
    "X_trainMM = scalerXMM.transform(X_train)\n",
    "y_trainMM = scaleryMM.transform(y_train)\n",
    "X_testMM = scalerXMM.transform(X_test)\n",
    "y_testMM = scaleryMM.transform(y_test)\n",
    "\n",
    "\n",
    "print(df_ml.iloc[7397][\"yield\"])\n",
    "print(y_testMM[0])\n",
    "print(scaleryMM.inverse_transform(y_testMM)[0])\n",
    "\n",
    "\n",
    "linearMM = LinearRegression()\n",
    "\n",
    "linearMM.fit(X_trainMM, y_trainMM)\n",
    "\n",
    "y_predMM = linearMM.predict(X_testMM)\n",
    "\n",
    "rmseMM = math.sqrt(mean_squared_error(y_testMM, y_predMM))\n",
    "rrmseMM = rmseMM / (0.5)\n",
    "r2MM = r2_score(y_testMM, y_predMM)\n",
    "print(rmseMM)\n",
    "print(rrmseMM)\n",
    "print(r2MM)\n",
    "\n",
    "\n",
    "plot_predictions(y_testMM, y_predMM, \"Linear Regression using MinMaxScaler\")\n",
    "\n",
    "\n",
    "regrMM = RandomForestRegressor(max_depth=20, random_state=0)\n",
    "\n",
    "\n",
    "regrMM.fit(X_trainMM, y_trainMM.ravel())\n",
    "\n",
    "y_predMM = regrMM.predict(X_testMM)\n",
    "rmseMM = math.sqrt(mean_squared_error(y_testMM, y_predMM))\n",
    "rrmseMM = rmseMM / (0.5)\n",
    "r2MM = r2_score(y_testMM, y_predMM)\n",
    "print(rmseMM)\n",
    "print(rrmseMM)\n",
    "print(r2MM)\n",
    "\n",
    "\n",
    "plot_predictions(y_testMM, y_predMM, \"Random Forest Regressor using MinMaxScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c93ec-4629-4b51-a502-18065851eea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
